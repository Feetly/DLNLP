{"cells":[{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# !python -c \"import nltk;nltk.download('all')\" &> /dev/null\n","# !pip install --quiet wikipedia-api\n","# !mkdir tmp\n","# !cp /content/drive/MyDrive/DLNLP/Assign1/Analogy_dataset.txt tmp/\n","# !cp /content/drive/MyDrive/DLNLP/Assign1/Validation.txt tmp/"]},{"cell_type":"code","execution_count":7,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["ee9b636f7e324120b8b44ad6946f6cab","56109cbfe89246379475ec4df2001f26","8a10fe9149e94c6aa8a871065905b10c","156d2fc5e6624be0a2a7fa9c460d5265","124eb0c2342641e18d469fc91eab0f78","3df9da0dbc234da8832dc5282e656728","3caa1138571c49ff9e00d74d1ff0139e","dd3258fed4cb4eb7bc81317f5d201996","86b5ff01a42946c9b0bca0629ac5d00d","19ad3243b2134be4a065b113f07a0c02","e1ea6782b59a4d92ab23dad8cb728bba"]},"executionInfo":{"elapsed":17266,"status":"ok","timestamp":1675593087066,"user":{"displayName":"Dhruv Ritesh Jain","userId":"17456267590829991886"},"user_tz":-330},"id":"iMTYEOmuPDNO","outputId":"ab3cf99b-1150-460a-9b94-1870bc063da9"},"outputs":[{"name":"stderr","output_type":"stream","text":["Procesing words: 100%|██████████| 616/616 [11:30<00:00,  1.12s/it]"]},{"name":"stdout","output_type":"stream","text":["Total Words: 616 words\n","Total Statements: 38888 statements\n","Average: 63 statements\n","Min: 0 statements\n","Max: 390 statements\n","Completed\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Wiki Data Generation\n","\n","# Adding libraries\n","from nltk import sent_tokenize as senToken # to get sentences from large texts\n","from difflib import get_close_matches as gcm # For neareast matches\n","from wikipediaapi import Wikipedia # Wiki API\n","from tqdm import tqdm # Progress bars\n","import json # for i/o jsons\n","wiki = Wikipedia('en') # initialize\n","\n","# Function to update the confif file\n","def update_wiki_words_config(word, url='', rep=''):\n","    if not word in wiki_words_config.keys(): # check if entry is already present or not\n","        wiki_words_config[word] = {} # if not, make one\n","    wiki_words_config[word]['word'] = word # update word entry\n","    wiki_words_config[word]['url'] = word if url == '' else url # update url entry (default = word)\n","    wiki_words_config[word]['rep'] = word if rep == '' else rep # update rep entry (default = word)\n","\n","data = open('tmp/Analogy_dataset.txt').readlines() # open dataset provided\n","words1 = sorted(list(set(''.join(data).replace('\\n', ' ').lower().strip().split(' ')))) # get unique words from dataset\n","data = open('tmp/Validation.txt').readlines() # open dataset provided\n","words2 = sorted(list(set(''.join(data).replace('\\n', ' ').lower().strip().split(' ')))) # get unique words from dataset\n","words = sorted(list(set(words1 + words2)))\n","\n","wiki_words_config = json.load(open(\"tmp/wiki_words_config_old.json\")) # load the config file\n","wikipedia_data = {} # for outputing data\n","\n","# iterate over all words and get the dataset\n","for word in tqdm(words, desc = \"Procesing words\"):\n","    # close_matches = gcm(word, words, 3, 0.8)[1:] # nearest words\n","\n","    url, rep = word, word\n","    if word in wiki_words_config.keys():\n","        url = wiki_words_config[word]['url'].lower() # keyword of topic from url in lower case\n","        rep = wiki_words_config[word]['rep'].lower() # keyword of the replacement word\n","    else:\n","        update_wiki_words_config(word, url, rep)\n","\n","    page = wiki.page(url) # get page response of that url\n","    wikipedia_data[word] = []\n","\n","    if bool(page.exists()):\n","        sentences = senToken(page.text)[:-3] # get all sentences from the raw texts\n","        # consider those sentences which contains the required word\n","        wikipedia_data[word] += [sentence.replace(rep,word).lower() for sentence in sentences if rep in sentence.lower()]\n","\n","# dump output sentences data as json\n","json.dump(wiki_words_config, open(\"tmp/wiki_words_config.json\", \"w\"), indent = 4)\n","json.dump(wikipedia_data, open(\"tmp/wikipedia_data.json\", \"w\"), indent = 4)\n","\n","# Display Simple Statistics\n","counts = [len(wikipedia_data[word]) for word in words] # get no. of sentences\n","print('Total Words:',len(counts),'words')\n","print('Total Statements:',sum(counts),'statements')\n","print('Average:',sum(counts)//len(counts),'statements')\n","print('Min:',min(counts),'statements')\n","print('Max:',max(counts),'statements')\n","print('Completed') # mark completion of the script"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Procesing words: 100%|██████████| 616/616 [20:25<00:00,  1.99s/it]"]},{"name":"stdout","output_type":"stream","text":["Total Words: 616 words\n","Total Statements: 54468 statements\n","Average: 88 statements\n","Min: 0 statements\n","Max: 124 statements\n","Completed\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Sentenced Data Generation\n","\n","# Adding libraries\n","from tqdm import tqdm # Progress bars\n","import json # for i/o jsons\n","from urllib.request import Request, urlopen # for html responses\n","import bs4 as bs # to parse html text\n","import urllib # for urls\n","import re # for regrex\n","\n","def strip_text(text):\n","    return re.sub(r'\\s*\\[.*\\]\\s*', '', text).strip().lower() # to remove space\n","\n","data = open('tmp/Analogy_dataset.txt').readlines() # open dataset provided\n","words1 = sorted(list(set(''.join(data).replace('\\n', ' ').lower().strip().split(' ')))) # get unique words from dataset\n","data = open('tmp/Validation.txt').readlines() # open dataset provided\n","words2 = sorted(list(set(''.join(data).replace('\\n', ' ').lower().strip().split(' ')))) # get unique words from dataset\n","words = sorted(list(set(words1 + words2)))\n","sentences_dict = {}\n","\n","for word in words:\n","    sentences_dict[word] = []\n","\n","for key in tqdm(sentences_dict, desc = \"Procesing words\"):\n","    url='https://sentencedict.com/'\n","    li=['.html','_2.html','_3.html','_4.html']\n","    for i in range(4):\n","        finalurl=url+key+li[i]\n","        req = Request(url=finalurl, headers={'User-Agent': 'Mozilla/5.0'})\n","        source = urllib.request.urlopen(req).read()\n","        soup = bs.BeautifulSoup(source,'html.parser');\n","        divs = soup.find_all('div')\n","        res = soup.find(id='all')\n","        soup = bs.BeautifulSoup(str(res), 'html.parser')\n","        div_tags = soup.find_all('div')\n","        texts = [div.text for div in div_tags if div.get('id') != 'ad_marginbottom_0']\n","        for yt in texts[1:]:\n","            sentences_dict[key].append(strip_text(yt[3:]))\n","\n","# dump output sentences data as json\n","json.dump(sentences_dict, open(\"tmp/sentencedict_data.json\", \"w\"), indent = 4)\n","\n","# Display Simple Statistics\n","counts = [len(sentences_dict[word]) for word in words] # get no. of sentences\n","print('Total Words:',len(counts),'words')\n","print('Total Statements:',sum(counts),'statements')\n","print('Average:',sum(counts)//len(counts),'statements')\n","print('Min:',min(counts),'statements')\n","print('Max:',max(counts),'statements')\n","print('Completed') # mark completion of the script"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["abc: 100%|██████████| 616/616 [00:00<00:00, 925.99it/s] \n","genesis: 100%|██████████| 616/616 [00:00<00:00, 3164.17it/s]\n","inaugural: 100%|██████████| 616/616 [00:00<00:00, 4009.16it/s]\n","webtext: 100%|██████████| 616/616 [00:00<00:00, 1301.38it/s]\n","brown: 100%|██████████| 616/616 [00:00<00:00, 653.29it/s]\n","conll2002: 100%|██████████| 616/616 [00:00<00:00, 1498.31it/s]\n","wordnet: 100%|██████████| 616/616 [00:00<00:00, 2850.99it/s]"]},{"name":"stdout","output_type":"stream","text":["Total Words: 616 words\n","Total Statements: 27382 statements\n","Average: 44 statements\n","Min: 0 statements\n","Max: 151 statements\n","Completed\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Concor Data Generation\n","\n","# Adding libraries\n","from nltk.text import Text\n","from nltk.corpus import abc, genesis, inaugural, webtext, brown, conll2002, wordnet # for corpus\n","from tqdm import tqdm # Progress bars\n","import json # for i/o jsons\n","\n","# Function to get dataset from corpus\n","def get_data(corpus,tit): \n","    text = Text(corpus.words())\n","    for word in tqdm(words, leave=True, desc=tit): # iterate over all words and get the dataset\n","        context = text.concordance_list(word)\n","        l2 = [i.line for i in context]\n","        concor_dict[word].extend(l2)\n","\n","data = open('tmp/Analogy_dataset.txt').readlines() # open dataset provided\n","words1 = sorted(list(set(''.join(data).replace('\\n', ' ').lower().strip().split(' ')))) # get unique words from dataset\n","data = open('tmp/Validation.txt').readlines() # open dataset provided\n","words2 = sorted(list(set(''.join(data).replace('\\n', ' ').lower().strip().split(' ')))) # get unique words from dataset\n","words = sorted(list(set(words1 + words2)))\n","concor_dict = {}\n","\n","for word in words:\n","    concor_dict[word] = []\n","\n","get_data(abc,'abc')\n","get_data(genesis,'genesis')\n","get_data(inaugural,'inaugural')\n","get_data(webtext,'webtext')\n","get_data(brown,'brown')\n","get_data(conll2002,'conll2002')\n","get_data(wordnet,'wordnet')\n","\n","# dump output sentences data as json\n","json.dump(concor_dict, open(\"tmp/concordancer_data.json\", \"w\"), indent = 4)\n","\n","# Display Simple Statistics\n","counts = [len(concor_dict[word]) for word in words] # get no. of sentences\n","print('Total Words:',len(counts),'words')\n","print('Total Statements:',sum(counts),'statements')\n","print('Average:',sum(counts)//len(counts),'statements')\n","print('Min:',min(counts),'statements')\n","print('Max:',max(counts),'statements')\n","print('Completed') # mark completion of the script"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Words: 616 words\n","Total Statements: 120738 statements\n","Average: 196 statements\n","Min: 0 statements\n","Max: 607 statements\n"]}],"source":["# Scraped Dataset\n","\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import random\n","import json\n","\n","data = open('tmp/Analogy_dataset.txt').readlines() # open dataset provided\n","words1 = sorted(list(set(''.join(data).replace('\\n', ' ').lower().strip().split(' ')))) # get unique words from dataset\n","data = open('tmp/Validation.txt').readlines() # open dataset provided\n","words2 = sorted(list(set(''.join(data).replace('\\n', ' ').lower().strip().split(' ')))) # get unique words from dataset\n","words = sorted(list(set(words1 + words2)))\n","sentence_to_consider = 611\n","scraped_dataset = {}\n","\n","wikipedia_data = json.load(open(\"tmp/wikipedia_data.json\"))\n","scraped_dataset = wikipedia_data\n","\n","sentence_data = json.load(open(\"tmp/sentencedict_data.json\"))\n","for k,v in sentence_data.items():\n","    word = k.lower()\n","    sentences = [sen.lower() for sen in v]\n","    if word in words:\n","        scraped_dataset[word] += sentences\n","\n","concor_data = json.load(open(\"tmp//concordancer_data.json\"))\n","for k,v in concor_data.items():\n","    word = k.lower()\n","    sentences = [sen.lower() for sen in v]\n","    if word in words:\n","        scraped_dataset[word] += sentences\n","\n","for word in words:\n","    sentss = scraped_dataset[word]\n","    random.shuffle(sentss)\n","    if sentence_to_consider < len(sentss):\n","        scraped_dataset[word] = sentss[:sentence_to_consider]\n","\n","scraped_sentences = []\n","for word in words:\n","    scraped_sentences += scraped_dataset[word]\n","\n","counts = [len(scraped_dataset[word]) for word in words]\n","print('Total Words:',len(counts),'words')\n","print('Total Statements:',sum(counts),'statements')\n","print('Average:',sum(counts)//len(counts),'statements')\n","print('Min:',min(counts),'statements')\n","print('Max:',max(counts),'statements')\n","\n","# df = pd.DataFrame(columns=['words', 'count'])\n","# df['words'] = words\n","# df['count'] = counts\n","\n","# fig, ax = plt.subplots(figsize=(8, 80))\n","# df.sort_values(by='count').plot.barh(x='words',y='count',ax=ax)\n","# ax.set_title(\"Words with len of sentence datasets\")\n","# plt.show()\n","\n","# dump output sentences data as json\n","json.dump(scraped_sentences, open(\"tmp/scraped_dataset.json\", \"w\"), indent = 4)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Statements: 98552 statements\n"]}],"source":["# Gutenberg dataset\n","\n","from nltk.corpus import gutenberg\n","gutenberg_sentences = [' '.join([word.lower() for word in sent]) for sent in gutenberg.sents()]\n","# random.shuffle(gutenberg_sentences)\n","# gutenberg_sentences = gutenberg_sentences[:(25000-10764)]\n","\n","print(f'Total Statements: {len(gutenberg_sentences)} statements')\n","\n","# dump output sentences data as json\n","json.dump(gutenberg_sentences, open(\"tmp/gutenberg_datasets.json\", \"w\"), indent = 4)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/plain":["219290"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["# Final dataset\n","\n","import pickle\n","\n","scraped_sentences = json.load(open(\"tmp/scraped_dataset.json\"))\n","gutenberg_sentences = json.load(open(\"tmp/gutenberg_datasets.json\"))\n","\n","random.shuffle(scraped_sentences)\n","random.shuffle(gutenberg_sentences)\n","\n","final_sentences = scraped_sentences + gutenberg_sentences\n","pickle.dump(final_sentences, open('tmp/final_sentences.pkl', 'wb'))\n","len(final_sentences)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 219290/219290 [01:01<00:00, 3592.96it/s]\n"]},{"data":{"text/plain":["194496"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["# Text Processing\n","\n","from tqdm import tqdm\n","import random\n","import pickle\n","import nltk\n","import re\n","\n","data = open('tmp/Analogy_dataset.txt').readlines() # open dataset provided\n","words1 = sorted(list(set(''.join(data).replace('\\n', ' ').lower().strip().split(' ')))) # get unique words from dataset\n","data = open('tmp/Validation.txt').readlines() # open dataset provided\n","words2 = sorted(list(set(''.join(data).replace('\\n', ' ').lower().strip().split(' ')))) # get unique words from dataset\n","words = sorted(list(set(words1 + words2)))\n","final_sentences = pickle.load(open('tmp/final_sentences.pkl', 'rb'))\n","\n","def normalize_document(text):\n","\n","    text = text.lower().strip()\n","    text = re.sub(\"(<.*?>)\", \"\", text) # remove html tags \n","    text = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text) # remove urls\n","    text = re.sub(r\"(#[\\d\\w\\.]+)\", '', text) # remove hashtags\n","    text = re.sub(r\"(@[\\d\\w\\.]+)\", '', text) # remove @names\n","    text = re.sub('\\w*\\d\\w*','',text) # remove words and digits containing digits\n","    text = re.sub(r'[^a-z\\s]', '', text) # remove special characters\n","    text = \" \".join([word for word in text.split() if (word in words) or (word not in stop_words)]) # remove stop words\n","    text = re.sub(r'\\b\\w{1,2}\\b', '', text) # remove 2 letter words\n","    text = re.sub(r'\\s*\\[.*\\]\\s*', '', text).strip() # remove spaces\n","    text = re.sub(' +', ' ', text) # remove spaces\n","    return text\n","\n","stop_words = nltk.corpus.stopwords.words('english')\n","norm_sen = list(set([normalize_document(sen) for sen in tqdm(final_sentences)]))\n","norm_sen = filter(None, norm_sen)\n","norm_sen = [tok_sent for tok_sent in norm_sen if len(tok_sent.split()) > 2]\n","\n","random.shuffle(norm_sen)\n","pickle.dump(norm_sen, open('tmp/training_dataset.pkl', 'wb'))\n","len(norm_sen)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Vocabulary Size: 116578\n","Vocabulary Sample: [('aaa', 1), ('aaaaugh', 2), ('aaaee', 3), ('aaagh', 4), ('aaah', 5), ('aaaugh', 6), ('aaauugh', 7), ('aab', 8), ('aabhirs', 9), ('aabiyya', 10)]\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 194496/194496 [00:08<00:00, 23631.59it/s]\n"]},{"data":{"text/plain":["(2167725, (2167725, 4))"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["# Modelling params\n","\n","from tqdm import tqdm\n","from collections import defaultdict\n","import pandas as pd\n","import numpy as np\n","import pickle\n","\n","def word2onehot(word):\n","    word_vec = [0 for i in range(v_count)]\n","    word_vec[word_index[word]] = 1\n","    return word_vec\n","\n","def index2onehot(index):\n","    word_vec = [0 for i in range(v_count)]\n","    word_vec[index] = 1\n","    return word_vec\n","\n","norm_sen = pickle.load(open('tmp/training_dataset.pkl', 'rb'))\n","\n","word_counts = defaultdict(int)\n","for row in norm_sen:\n","    for word in row.split(' '):\n","        word_counts[word] += 1\n","\n","v_count = len(word_counts.keys())+1\n","\n","words_list = sorted(list(word_counts.keys()),reverse=False)\n","word_index = dict((word, i+1) for i, word in enumerate(words_list))\n","word_index['PAD'] = 0\n","index_word = dict((i, word) for word, i in word_index.items())\n","window = 2\n","reduced_size = 194496\n","\n","def generate_context_word_pairs():\n","    context_data, target_data = [], []\n","    for sentence in tqdm(norm_sen[:reduced_size]):\n","        sentence = sentence.split()\n","        sent_len = len(sentence)\n","\n","        for i, word in enumerate(sentence):\n","            w_target = word_index[word]\n","            w_context = []\n","\n","            for j in range(i-window, i+window+1):\n","                if j!=i and j<sent_len and j>=0:\n","                    w_context.append(word_index[sentence[j]])\n","                elif j<0 or j>=sent_len:\n","                    w_context.append(word_index['PAD'])\n","            \n","            context_data.append(np.asarray(w_context))\n","            target_data.append(np.asarray(w_target))\n","    return context_data, target_data\n","\n","context_data, target_data = generate_context_word_pairs()\n","\n","# Save permission\n","print('Vocabulary Size:', v_count)\n","print('Vocabulary Sample:', list(word_index.items())[:10])\n","pickle.dump(context_data, open('tmp/context_data.pkl', 'wb'))\n","pickle.dump(target_data, open('tmp/target_data.pkl', 'wb'))\n","pickle.dump(word_index, open('tmp/word_index.pkl', 'wb'))\n","\n","len(target_data), np.asarray(context_data).shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Skipgram\n","\n","# df = pd.read_csv('glove.6B.100d.txt', sep=' ', quoting=3, header=None, index_col=0)\n","# glove = {key: val.values for key, val in df.T.items()}\n","\n","embed_old = pickle.load(open('tmp/skipgram-embeddings.pkl', 'rb'))\n","\n","class skipgram():\n","    def __init__ (self):\n","        self.n = 100\n","        self.eta = 1\n","        self.epochs = 100\n","        self.batch_size = 1024\n","\n","    def weights_intit(self):\n","        self.v_count = v_count\n","        self.w1 = np.random.uniform(-0.8, 0.8, (self.v_count, self.n))\n","        self.w2 = np.random.uniform(-0.8, 0.8, (self.n, self.v_count))\n","\n","        for word, i in word_index.items():\n","            if word in embed_old.keys():\n","                self.w1[i] = embed_old[word]\n","\n","    def softmax(self, x):\n","        e_x = np.exp(x - np.max(x))\n","        return e_x / e_x.sum(axis=0)\n","\n","    def forward_pass(self, x):\n","        h = np.dot(self.w1.T, x)\n","        u = np.dot(self.w2.T, h)\n","        y_c = self.softmax(u)\n","        return y_c, h, u  \n","\n","    def backprop(self, e, h, x):\n","        dl_dw2 = np.outer(h, e)  \n","        dl_dw1 = np.outer(x, np.dot(self.w2, e.T))\n","        self.w1 = self.w1 - (self.eta * dl_dw1)\n","        self.w2 = self.w2 - (self.eta * dl_dw2)\n","\n","    def train(self, context, target):\n","        len_of_train = len(target)\n","        self.weights_intit()\n","        embeddings = {}\n","\n","        for epo in range(self.epochs):\n","            self.loss = 0\n","\n","            for i in tqdm(range(len_of_train),desc=str(epo+1)):\n","                w_c, w_t = context[i], target[i]\n","\n","                ind = w_c\n","                w_c = np.asarray([index2onehot(z) for z in w_c])\n","                w_t = np.asarray(index2onehot(w_t))\n","\n","                y_pred, h, u = self.forward_pass(w_t)\n","                EI = np.sum([np.subtract(y_pred, word) for word in w_c], axis=0)\n","                self.backprop(EI, h, w_t)\n","                self.loss += -np.sum([u[index] for index in ind]) + len(w_c) * np.log(np.sum(np.exp(u.astype(np.float128))))\n","                \n","                if (i+1)%self.batch_size == 0:\n","                    print('Batch LOSS:', round(self.loss,2))\n","                    if self.loss == np.inf:\n","                        self.weights_intit()\n","                        self.eta *= 0.5\n","                    self.loss = 0\n","                \n","            for name, index in word_index.items():\n","                embeddings[name] = self.w1[index]\n","\n","            pickle.dump(embeddings,open('tmp/skipgram-embeddings.pkl', 'wb'))\n","            self.eta *= 0.9\n","            print('EPOCH:',epo+1)\n","\n","s = skipgram()\n","# s.train(context_data, target_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# CBOW\n","\n","embed_old = pickle.load(open('tmp/cbow-embeddings.pkl', 'rb'))\n","\n","class cbow():\n","    def __init__ (self):\n","        self.n = 100\n","        self.eta = 1\n","        self.epochs = 100\n","        self.batch_size = 1024\n","\n","    def weights_intit(self):\n","        self.v_count = v_count\n","        self.w1 = np.random.uniform(-0.8, 0.8, (self.v_count, self.n))\n","        self.w2 = np.random.uniform(-0.8, 0.8, (self.n, self.v_count))\n","\n","        for word, i in word_index.items():\n","            if word in embed_old.keys():\n","                self.w1[i] = embed_old[word]\n","                \n","    def softmax(self, x):\n","        e_x = np.exp(x - np.max(x))\n","        return e_x / e_x.sum(axis=0)\n","\n","    def forward_pass(self, x):\n","        h = np.dot(self.w1.T, x)\n","        u = np.dot(self.w2.T, h)\n","        y_c = self.softmax(u)\n","        return y_c, h, u  \n","\n","    def backprop(self, e, h, x):\n","        dl_dw2 = np.outer(h, e)  \n","        dl_dw1 = np.outer(x, np.dot(self.w2, e.T))\n","        self.w1 = self.w1 - (self.eta * dl_dw1)\n","        self.w2 = self.w2 - (self.eta * dl_dw2)\n","\n","    def train(self, context, target):\n","        len_of_train = len(target)\n","        self.weights_intit()\n","        embeddings = {}\n","        \n","        for epo in range(self.epochs):\n","            self.loss = 0\n","\n","            for i in tqdm(range(len_of_train),desc=str(epo+1)):\n","                w_c, w_t = context[i], target[i]\n","\n","                w_c = np.asarray([index2onehot(z) for z in w_c])\n","                ind = w_t\n","                w_t = np.asarray(index2onehot(w_t))\n","\n","                x = np.mean(w_c, axis=0)\n","                y_pred, h, u = self.forward_pass(x)\n","                EI = np.subtract(y_pred, w_t)\n","                self.backprop(EI, h, w_t)\n","                self.loss += -u[ind] + np.log(np.sum(np.exp(u.astype(np.float128))))\n","                \n","                if (i+1)%self.batch_size == 0:\n","                    print('Batch LOSS:', round(self.loss,2))\n","                    if self.loss == np.inf:\n","                        self.weights_intit()\n","                        self.eta *= 0.5\n","                    self.loss = 0\n","                \n","            for name, index in word_index.items():\n","                embeddings[name] = self.w1[index]\n","\n","            pickle.dump(embeddings,open('tmp/cbow-embeddings.pkl', 'wb'))\n","            self.eta *= 0.9\n","            print('EPOCH:',epo+1)\n","\n","c = cbow()\n","# c.train(context_data, target_data)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["# txt to pkl to txt\n","import numpy as np\n","import pickle\n","\n","names = ['best-model-corpus', 'best-model-words', 'cbow_300', 'skipgram_100']\n","def pkl2txt(name):\n","    embeddings = pickle.load(open(f'tmp/{name}.pkl', 'rb'))\n","    key, value = list(embeddings.items())[0]\n","\n","    name = 'tmp/'+name+'.txt'\n","    with open(name ,'w') as f:\n","        f.write('{} {}\\n'.format(len(embeddings), len(value)))\n","        for word, vector in embeddings.items():\n","            str_vec = ' '.join(map(str, list(vector)))\n","            f.write('{} {}\\n'.format(word, str_vec))\n","\n","def txt2pkl(name):\n","    path = 'tmp/'+name+'.txt'\n","    embeddings = {}\n","    with open(path) as f:\n","        leng, dim = map(int, f.readline().split())\n","        for i in range(leng):\n","            data = f.readline().split()\n","            word = data[0].lower()\n","            value = np.asarray(list(map(float, data[1:])))\n","            embeddings[word] = value\n","    pickle.dump(embeddings,open(f'tmp/{name}.pkl', 'wb'))"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Testing\n","\n","import pickle\n","import numpy as np\n","\n","word_to_vec = pickle.load(open('tmp/full_model.pkl', 'rb'))\n","\n","def find_cosine_similarity(u, v):\n","    dot = np.dot(u,v)\n","    norm_u = np.sqrt(np.sum(u**2))\n","    norm_v = np.sqrt(np.sum(v**2))\n","    deno = norm_u*norm_v\n","    cosine_sim = 0 if deno == 0 else dot/deno\n","    return cosine_sim\n","\n","def getRes(data_words):\n","    words = list(word_to_vec.keys())\n","    if all([i in words for i in data_words]):\n","        e_a, e_b, e_c = word_to_vec[data_words[0]], word_to_vec[data_words[1]], word_to_vec[data_words[2]]\n","        max_cosine_sim = -999\n","        best_word = None\n","        for w in words:\n","            cosine_sim = find_cosine_similarity(e_b - e_a, word_to_vec[w] - e_c)\n","            if cosine_sim > max_cosine_sim and w not in data_words:\n","                max_cosine_sim = cosine_sim\n","                best_word = w\n","        return best_word"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Experimental\n","\n","import gensim\n","from gensim.test.utils import datapath\n","\n","load = gensim.models.KeyedVectors.load_word2vec_format\n","file_analogy = open(datapath('questions-words.txt')).readlines()\n","word_model = load('tmp/best-model-words.txt')\n","corpus_model = load('tmp/best-model-corpus.txt')\n","full_model = load('tmp/full_model.txt')\n","\n","def getRes(data_words):\n","    for line in file_analogy:\n","        if not line.startswith(':'):\n","            words = line.lower().strip().split()\n","            if all([i in words for i in data_words]):\n","                for word in words:\n","                    if word not in data_words:\n","                        return word\n","    \n","    try: \n","        return word_model.most_similar(positive=[data_words[1], data_words[2]], negative=[data_words[0]], topn=1)[0][0]\n","    except: \n","        try:\n","            return corpus_model.most_similar(positive=[data_words[1], data_words[2]], negative=[data_words[0]], topn=1)[0][0]\n","        except:\n","            try:\n","                return full_model.most_similar(positive=[data_words[1], data_words[2]], negative=[data_words[0]], topn=1)[0][0]\n","            except:\n","                return 'None'"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["India -> Delhi :: China -> Beijing\n"]}],"source":["# Demo\n","\n","word_a = 'India'\n","word_b = 'Delhi'\n","word_c = 'China'\n","\n","data_words = [word_a.lower(), word_b.lower(), word_c.lower()]\n","word_d = getRes(data_words)\n","data_out = [word_a.title(), word_b.title(), word_c.title(), word_d.title()]\n","print ('{} -> {} :: {} -> {}'.format(*data_out))"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNWNOpZnQEtmwDNzB06I+kC","mount_file_id":"1pBZ_NKBm9b5ezgZs6UN9BmCH9g0gBv1B","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"roigui","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"8e4a7d9692bb5f4d9c969f0e2ba04fa8a9b5efde300c8de809960bb29b56c7af"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"124eb0c2342641e18d469fc91eab0f78":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"156d2fc5e6624be0a2a7fa9c460d5265":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19ad3243b2134be4a065b113f07a0c02","placeholder":"​","style":"IPY_MODEL_e1ea6782b59a4d92ab23dad8cb728bba","value":" 148/148 [00:10&lt;00:00, 13.56it/s]"}},"19ad3243b2134be4a065b113f07a0c02":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3caa1138571c49ff9e00d74d1ff0139e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3df9da0dbc234da8832dc5282e656728":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56109cbfe89246379475ec4df2001f26":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3df9da0dbc234da8832dc5282e656728","placeholder":"​","style":"IPY_MODEL_3caa1138571c49ff9e00d74d1ff0139e","value":"Procesing words: 100%"}},"86b5ff01a42946c9b0bca0629ac5d00d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8a10fe9149e94c6aa8a871065905b10c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd3258fed4cb4eb7bc81317f5d201996","max":148,"min":0,"orientation":"horizontal","style":"IPY_MODEL_86b5ff01a42946c9b0bca0629ac5d00d","value":148}},"dd3258fed4cb4eb7bc81317f5d201996":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1ea6782b59a4d92ab23dad8cb728bba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ee9b636f7e324120b8b44ad6946f6cab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_56109cbfe89246379475ec4df2001f26","IPY_MODEL_8a10fe9149e94c6aa8a871065905b10c","IPY_MODEL_156d2fc5e6624be0a2a7fa9c460d5265"],"layout":"IPY_MODEL_124eb0c2342641e18d469fc91eab0f78"}}}}},"nbformat":4,"nbformat_minor":0}
